{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ouyan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def handle_nan(data):\n",
    "    data2 = data.fillna(\" \")\n",
    "    return data2\n",
    "\n",
    "def wordopt(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)  \n",
    "    return text\n",
    "\n",
    "# remove stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = re.sub('[^a-zA-Z]',' ',text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [ps.stem(word) for word in text if not word in stopwords.words('english')]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "def namestr(obj, namespace):\n",
    "    return [name for name in namespace if namespace[name] is obj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read file and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"datasets/train.csv\")\n",
    "predict = pd.read_csv(\"datasets/predict.csv\")\n",
    "\n",
    "#train = train[0:100]\n",
    "#predict = predict[0:100]\n",
    "\n",
    "train = handle_nan(train)\n",
    "predict = handle_nan(predict)\n",
    "\n",
    "x = train['text'].apply(wordopt)\n",
    "y = train['label']\n",
    "p = predict['text'].apply(wordopt)\n",
    "\n",
    "#x = train['text'].apply(stemming)\n",
    "#p = predict['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn prepare functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "vectorization = CountVectorizer()\n",
    "x_train = vectorization.fit_transform(x_train)\n",
    "x_test = vectorization.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_testing(model, news):\n",
    "    testing_news = {\"text\":[news]}\n",
    "    new_def_test = pd.DataFrame(testing_news)\n",
    "    new_x_test = new_def_test[\"text\"]\n",
    "    new_x_test = vectorization.transform(new_x_test)\n",
    "    return \"proba of 0 & 1:\", model.predict_proba(new_x_test)\n",
    "\n",
    "def preciction(model, data):\n",
    "    model_name = namestr(model,globals())[0]\n",
    "    new_def_test = p\n",
    "    new_x_test = vectorization.transform(new_def_test)\n",
    "\n",
    "    predictions_test = pd.DataFrame(model.predict(new_x_test))\n",
    "    predictions_test_predict_proba = pd.DataFrame(model.predict_proba(new_x_test))\n",
    "    test_id = pd.DataFrame(predict[\"id\"])\n",
    "\n",
    "    submission = pd.concat([test_id, predictions_test, predictions_test_predict_proba],axis=1)\n",
    "    submission.columns = [\"id\",model_name,\"proba_0\",\"proba_1\"]\n",
    "    submission.to_csv(\"out_{}.csv\".format(model_name),index=False)\n",
    "\n",
    "def cross_val(model):\n",
    "    scores = cross_val_score(model, x_train,y_train, scoring=\"neg_mean_squared_error\", cv=2)\n",
    "    tree_rmse_scores = np.sqrt(-scores)\n",
    "    return (\"Scores:\", scores),(\"Mean:\", scores.mean()),(\"Standard deviation:\", scores.std())\n",
    "\n",
    "def Confusion_Matrix_f1_score(model):    \n",
    "    y_train_pred = cross_val_predict(model, x_train,y_train, cv=3)\n",
    "    return confusion_matrix(y_train,y_train_pred), f1_score(y_train,y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare different sklearn algos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[7925,  397],\n",
       "        [1471, 6847]], dtype=int64),\n",
       " 0.8799640149081095)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "Bayesian = MultinomialNB()\n",
    "Bayesian.fit(x_train,y_train)\n",
    "Bayesian.score(x_test, y_test)\n",
    "Confusion_Matrix_f1_score(Bayesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('proba of 0 & 1:', array([[1.00000000e+00, 1.23440982e-15]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preciction(Bayesian,p)\n",
    "manual_testing(Bayesian, str(\"Not quite all the action is  . Peter Thiel, a founder of PayPal and Palantir who was the first outside investor in Facebook, spoke at the Republican convention in July. The New York Times reported on Saturday that Mr. Thiel is giving $1. 25 million to support Mr. Trump’s candidacy even as other supporters flee. (He also recently gave $1 million to a “super PAC” that supports Senator Rob Portman, the Republican freshman running for   in Ohio.) Getting involved in politics used to be seen as clashing with Silicon Valley’s value system: You transform the world by making problems obsolete, not solving them through Washington. Nor did entrepreneurs want to alienate whatever segment of customers did not agree with them politically. Such reticence is no longer in style here. “We’re a bunch of nerds not used to having a lot of limelight,” said Dave McClure, an investor who runs a tech incubator called 500 Startups.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[7878,  444],\n",
       "        [ 375, 7943]], dtype=int64),\n",
       " 0.9509727626459143)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Logistic = LogisticRegression()\n",
    "Logistic.fit(x_train,y_train)\n",
    "Confusion_Matrix_f1_score(Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('proba of 0 & 1:', array([[0.90491035, 0.09508965]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preciction(Logistic,p)\n",
    "manual_testing(Logistic, str(\"Not quite all the action is  . Peter Thiel, a founder of PayPal and Palantir who was the first outside investor in Facebook, spoke at the Republican convention in July. The New York Times reported on Saturday that Mr. Thiel is giving $1. 25 million to support Mr. Trump’s candidacy even as other supporters flee. (He also recently gave $1 million to a “super PAC” that supports Senator Rob Portman, the Republican freshman running for   in Ohio.) Getting involved in politics used to be seen as clashing with Silicon Valley’s value system: You transform the world by making problems obsolete, not solving them through Washington. Nor did entrepreneurs want to alienate whatever segment of customers did not agree with them politically. Such reticence is no longer in style here. “We’re a bunch of nerds not used to having a lot of limelight,” said Dave McClure, an investor who runs a tech incubator called 500 Startups.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[7317, 1005],\n",
       "        [ 906, 7412]], dtype=int64),\n",
       " 0.885808186435614)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "DecisionTree = DecisionTreeClassifier()\n",
    "DecisionTree.fit(x_train,y_train)\n",
    "Confusion_Matrix_f1_score(DecisionTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('proba of 0 & 1:', array([[1., 0.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preciction(DecisionTree,p)\n",
    "manual_testing(DecisionTree, str(\"Not quite all the action is  . Peter Thiel, a founder of PayPal and Palantir who was the first outside investor in Facebook, spoke at the Republican convention in July. The New York Times reported on Saturday that Mr. Thiel is giving $1. 25 million to support Mr. Trump’s candidacy even as other supporters flee. (He also recently gave $1 million to a “super PAC” that supports Senator Rob Portman, the Republican freshman running for   in Ohio.) Getting involved in politics used to be seen as clashing with Silicon Valley’s value system: You transform the world by making problems obsolete, not solving them through Washington. Nor did entrepreneurs want to alienate whatever segment of customers did not agree with them politically. Such reticence is no longer in style here. “We’re a bunch of nerds not used to having a lot of limelight,” said Dave McClure, an investor who runs a tech incubator called 500 Startups.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6704, 1618],\n",
       "        [1363, 6955]], dtype=int64),\n",
       " 0.8235154816174294)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "k_neighbor = KNeighborsClassifier()\n",
    "k_neighbor.fit(x_train,y_train)\n",
    "Confusion_Matrix_f1_score(k_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('proba of 0 & 1:', array([[0.2, 0.8]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preciction(k_neighbor,p)\n",
    "manual_testing(k_neighbor, str(\"Not quite all the action is  . Peter Thiel, a founder of PayPal and Palantir who was the first outside investor in Facebook, spoke at the Republican convention in July. The New York Times reported on Saturday that Mr. Thiel is giving $1. 25 million to support Mr. Trump’s candidacy even as other supporters flee. (He also recently gave $1 million to a “super PAC” that supports Senator Rob Portman, the Republican freshman running for   in Ohio.) Getting involved in politics used to be seen as clashing with Silicon Valley’s value system: You transform the world by making problems obsolete, not solving them through Washington. Nor did entrepreneurs want to alienate whatever segment of customers did not agree with them politically. Such reticence is no longer in style here. “We’re a bunch of nerds not used to having a lot of limelight,” said Dave McClure, an investor who runs a tech incubator called 500 Startups.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[7128, 1194],\n",
       "        [ 483, 7835]], dtype=int64),\n",
       " 0.9033262235545051)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#14 min to run\n",
    "from sklearn.svm import SVC\n",
    "SVC1 = SVC()\n",
    "SVC1.fit(x_train,y_train)\n",
    "Confusion_Matrix_f1_score(SVC1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[7918,  404],\n",
       "        [ 431, 7887]], dtype=int64),\n",
       " 0.9497260521404058)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#33 min to run\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp  = MLPClassifier(solver='lbfgs', activation='logistic')\n",
    "mlp.fit(x_train,y_train)\n",
    "Confusion_Matrix_f1_score(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('proba of 0 & 1:', array([[9.99999934e-01, 6.61538942e-08]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preciction(mlp,p)\n",
    "manual_testing(mlp, str(\"Not quite all the action is  . Peter Thiel, a founder of PayPal and Palantir who was the first outside investor in Facebook, spoke at the Republican convention in July. The New York Times reported on Saturday that Mr. Thiel is giving $1. 25 million to support Mr. Trump’s candidacy even as other supporters flee. (He also recently gave $1 million to a “super PAC” that supports Senator Rob Portman, the Republican freshman running for   in Ohio.) Getting involved in politics used to be seen as clashing with Silicon Valley’s value system: You transform the world by making problems obsolete, not solving them through Washington. Nor did entrepreneurs want to alienate whatever segment of customers did not agree with them politically. Such reticence is no longer in style here. “We’re a bunch of nerds not used to having a lot of limelight,” said Dave McClure, an investor who runs a tech incubator called 500 Startups.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "#Deep learning libraries\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_predict_tf(model_name, str):\n",
    "    data = []\n",
    "    data.append(pred)\n",
    "    p2 = pd.DataFrame(data)\n",
    "\n",
    "    p2 = p2[0].apply(wordopt)\n",
    "\n",
    "    tokenizer_p = Tokenizer(num_words = voc_size)\n",
    "    tokenizer_p.fit_on_texts(p2)\n",
    "\n",
    "    p2 = tokenizer_p.texts_to_sequences(p2)\n",
    "\n",
    "    p2 = tf.keras.preprocessing.sequence.pad_sequences(p2, padding='post', maxlen=sent_length)\n",
    "\n",
    "    return model_name.predict(p2)\n",
    "\n",
    "def preciction_tf(model, data):\n",
    "    model_name = namestr(model,globals())[0]\n",
    "    predictions_test = pd.DataFrame(model.predict(data))\n",
    "    test_id = pd.DataFrame(predict[\"id\"])\n",
    "\n",
    "    submission = pd.concat([test_id, predictions_test],axis=1)\n",
    "    submission.columns = [\"id\",model_name]\n",
    "    submission.to_csv(\"out_{}.csv\".format(model_name),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "voc_size = 5000\n",
    "\n",
    "x_train_2=[one_hot(words,voc_size)for words in x_train_1] \n",
    "x_test_2=[one_hot(words,voc_size)for words in x_test_1] \n",
    "p_2=[one_hot(words,voc_size)for words in p] \n",
    "\n",
    "\n",
    "#tokenizer = Tokenizer(num_words=voc_size)\n",
    "#tokenizer.fit_on_texts(x_train_1)\n",
    "#\n",
    "#x_train_2 = tokenizer.texts_to_sequences(x_train_1)\n",
    "#x_test_2 = tokenizer.texts_to_sequences(x_test_1)\n",
    "#\n",
    "#tokenizer_p = Tokenizer(num_words=voc_size)\n",
    "#tokenizer_p.fit_on_texts(p)\n",
    "#p_2 = tokenizer_p.texts_to_sequences(p)\n",
    "\n",
    "sent_length=5000\n",
    "x_train_3 = tf.keras.preprocessing.sequence.pad_sequences(x_train_2, padding='post', maxlen=sent_length)\n",
    "x_test_3 = tf.keras.preprocessing.sequence.pad_sequences(x_test_2, padding='post', maxlen=sent_length)\n",
    "p_3 = tf.keras.preprocessing.sequence.pad_sequences(p_2, padding='post', maxlen=sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "234/234 [==============================] - 314s 1s/step - loss: 0.6617 - accuracy: 0.7169 - val_loss: 0.4142 - val_accuracy: 0.8822\n",
      "Epoch 2/50\n",
      "234/234 [==============================] - 311s 1s/step - loss: 0.2987 - accuracy: 0.9230 - val_loss: 0.4867 - val_accuracy: 0.8600\n",
      "Epoch 3/50\n",
      "234/234 [==============================] - 328s 1s/step - loss: 0.4309 - accuracy: 0.8339 - val_loss: 0.3644 - val_accuracy: 0.8768\n",
      "Epoch 4/50\n",
      "234/234 [==============================] - 310s 1s/step - loss: 0.3178 - accuracy: 0.9009 - val_loss: 0.2823 - val_accuracy: 0.9062\n",
      "Epoch 5/50\n",
      "234/234 [==============================] - 309s 1s/step - loss: 0.8431 - accuracy: 0.8597 - val_loss: 0.8131 - val_accuracy: 0.8900\n",
      "Epoch 6/50\n",
      "234/234 [==============================] - 310s 1s/step - loss: 0.2482 - accuracy: 0.9322 - val_loss: 0.6126 - val_accuracy: 0.8852\n",
      "Epoch 7/50\n",
      "234/234 [==============================] - 310s 1s/step - loss: 0.2346 - accuracy: 0.9466 - val_loss: 0.5046 - val_accuracy: 0.8906\n",
      "Epoch 8/50\n",
      "234/234 [==============================] - 311s 1s/step - loss: 0.4899 - accuracy: 0.8300 - val_loss: 0.5742 - val_accuracy: 0.8287\n",
      "Epoch 9/50\n",
      "234/234 [==============================] - 311s 1s/step - loss: 0.4149 - accuracy: 0.8214 - val_loss: 0.4893 - val_accuracy: 0.8456\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(voc_size, 32))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "rnn = build_model()\n",
    "rnn.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "history = rnn.fit(x_train_3, y_train_1, epochs=50, validation_split=0.1, batch_size=64, shuffle=True, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loss, test acc\n",
    "rnn.evaluate(x_test_3, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = str(\n",
    "    \"Not quite all the action is  . Peter Thiel, a founder of PayPal and Palantir who was the first outside investor in Facebook, spoke at the Republican convention in July. The New York Times reported on Saturday that Mr. Thiel is giving $1. 25 million to support Mr. Trump’s candidacy even as other supporters flee. (He also recently gave $1 million to a “super PAC” that supports Senator Rob Portman, the Republican freshman running for   in Ohio.) Getting involved in politics used to be seen as clashing with Silicon Valley’s value system: You transform the world by making problems obsolete, not solving them through Washington. Nor did entrepreneurs want to alienate whatever segment of customers did not agree with them politically. Such reticence is no longer in style here. “We’re a bunch of nerds not used to having a lot of limelight,” said Dave McClure, an investor who runs a tech incubator called 500 Startups.\"\n",
    "    )\n",
    "manual_predict_tf(rnn, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preciction_tf(rnn, p_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using hyperameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 26m 08s]\n",
      "val_loss: 0.3319251239299774\n",
      "\n",
      "Best val_loss So Far: 0.3038800060749054\n",
      "Total elapsed time: 02h 12m 01s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(voc_size, 32))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,  return_sequences=True)))\n",
    "    model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)))\n",
    "    model.add(tf.keras.layers.Dense(hp.Int(\"dense\",16,256,16), activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    overwrite = True,\n",
    "    objective='val_loss',\n",
    "    max_trials=5)\n",
    "\n",
    "tuner.search(x_train_3, y_train_1, epochs=5, batch_size=64, validation_split=0.1)\n",
    "keras_tuner_best = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130/130 [==============================] - 75s 555ms/step - loss: 0.2997 - accuracy: 0.8976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2997337281703949, 0.8975961804389954]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test loss, test acc\n",
    "keras_tuner_best.evaluate(x_test_3, y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.2843911]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = str(\n",
    "    \"Not quite all the action is  . Peter Thiel, a founder of PayPal and Palantir who was the first outside investor in Facebook, spoke at the Republican convention in July. The New York Times reported on Saturday that Mr. Thiel is giving $1. 25 million to support Mr. Trump’s candidacy even as other supporters flee. (He also recently gave $1 million to a “super PAC” that supports Senator Rob Portman, the Republican freshman running for   in Ohio.) Getting involved in politics used to be seen as clashing with Silicon Valley’s value system: You transform the world by making problems obsolete, not solving them through Washington. Nor did entrepreneurs want to alienate whatever segment of customers did not agree with them politically. Such reticence is no longer in style here. “We’re a bunch of nerds not used to having a lot of limelight,” said Dave McClure, an investor who runs a tech incubator called 500 Startups.\"\n",
    "    )\n",
    "manual_predict_tf(keras_tuner_best, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 91s 559ms/step\n"
     ]
    }
   ],
   "source": [
    "preciction_tf(keras_tuner_best, p_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# autokeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autokeras as ak\n",
    "x_train_ak = x_train_1.to_numpy()\n",
    "y_train_ak = y_train_1.to_numpy()\n",
    "x_test_ak = x_test_1.to_numpy()\n",
    "y_test_ak = y_test_1.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\text_classifier\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\text_classifier\\tuner0.json\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "bert              |vanilla           |text_block_1/block_type\n",
      "0                 |0                 |classification_head_1/dropout\n",
      "adam_weight_decay |adam              |optimizer\n",
      "2e-05             |0.001             |learning_rate\n",
      "512               |None              |text_block_1/bert_block_1/max_sequence_length\n",
      "20000             |5000              |text_block_1/max_tokens\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 158, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 146, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\graph.py\", line 250, in build\n",
      "    outputs = block.build(hp, inputs=temp_inputs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 159, in build\n",
      "    output_node = self._build_block(hp, output_node, block_type)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 174, in _build_block\n",
      "    output_node = basic.BertBlock().build(hp, output_node)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\basic.py\", line 984, in build\n",
      "    bert_encoder.load_pretrained_weights()\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\keras_layers.py\", line 292, in load_pretrained_weights\n",
      "    path = keras.utils.get_file(\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 308, in get_file\n",
      "    _extract_archive(fpath, datadir, archive_format)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 137, in _extract_archive\n",
      "    archive.extractall(path)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2045, in extractall\n",
      "    self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2086, in extract\n",
      "    self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2159, in _extract_member\n",
      "    self.makefile(tarinfo, targetpath)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2208, in makefile\n",
      "    copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 247, in copyfileobj\n",
      "    buf = src.read(bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 506, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 0/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 158, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 146, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\graph.py\", line 250, in build\n",
      "    outputs = block.build(hp, inputs=temp_inputs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 159, in build\n",
      "    output_node = self._build_block(hp, output_node, block_type)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 174, in _build_block\n",
      "    output_node = basic.BertBlock().build(hp, output_node)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\basic.py\", line 984, in build\n",
      "    bert_encoder.load_pretrained_weights()\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\keras_layers.py\", line 292, in load_pretrained_weights\n",
      "    path = keras.utils.get_file(\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 308, in get_file\n",
      "    _extract_archive(fpath, datadir, archive_format)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 137, in _extract_archive\n",
      "    archive.extractall(path)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2045, in extractall\n",
      "    self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2086, in extract\n",
      "    self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2159, in _extract_member\n",
      "    self.makefile(tarinfo, targetpath)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2208, in makefile\n",
      "    copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 247, in copyfileobj\n",
      "    buf = src.read(bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 506, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 158, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 146, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\graph.py\", line 250, in build\n",
      "    outputs = block.build(hp, inputs=temp_inputs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 159, in build\n",
      "    output_node = self._build_block(hp, output_node, block_type)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 174, in _build_block\n",
      "    output_node = basic.BertBlock().build(hp, output_node)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\basic.py\", line 984, in build\n",
      "    bert_encoder.load_pretrained_weights()\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\keras_layers.py\", line 292, in load_pretrained_weights\n",
      "    path = keras.utils.get_file(\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 308, in get_file\n",
      "    _extract_archive(fpath, datadir, archive_format)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 137, in _extract_archive\n",
      "    archive.extractall(path)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2045, in extractall\n",
      "    self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2086, in extract\n",
      "    self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2159, in _extract_member\n",
      "    self.makefile(tarinfo, targetpath)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2208, in makefile\n",
      "    copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 247, in copyfileobj\n",
      "    buf = src.read(bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 506, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 158, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 146, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\graph.py\", line 250, in build\n",
      "    outputs = block.build(hp, inputs=temp_inputs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 159, in build\n",
      "    output_node = self._build_block(hp, output_node, block_type)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 174, in _build_block\n",
      "    output_node = basic.BertBlock().build(hp, output_node)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\basic.py\", line 984, in build\n",
      "    bert_encoder.load_pretrained_weights()\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\keras_layers.py\", line 292, in load_pretrained_weights\n",
      "    path = keras.utils.get_file(\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 308, in get_file\n",
      "    _extract_archive(fpath, datadir, archive_format)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 137, in _extract_archive\n",
      "    archive.extractall(path)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2045, in extractall\n",
      "    self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2086, in extract\n",
      "    self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2159, in _extract_member\n",
      "    self.makefile(tarinfo, targetpath)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2208, in makefile\n",
      "    copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 247, in copyfileobj\n",
      "    buf = src.read(bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 506, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 158, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 146, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\graph.py\", line 250, in build\n",
      "    outputs = block.build(hp, inputs=temp_inputs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 159, in build\n",
      "    output_node = self._build_block(hp, output_node, block_type)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 174, in _build_block\n",
      "    output_node = basic.BertBlock().build(hp, output_node)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\basic.py\", line 984, in build\n",
      "    bert_encoder.load_pretrained_weights()\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\keras_layers.py\", line 292, in load_pretrained_weights\n",
      "    path = keras.utils.get_file(\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 308, in get_file\n",
      "    _extract_archive(fpath, datadir, archive_format)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 137, in _extract_archive\n",
      "    archive.extractall(path)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2045, in extractall\n",
      "    self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2086, in extract\n",
      "    self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2159, in _extract_member\n",
      "    self.makefile(tarinfo, targetpath)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2208, in makefile\n",
      "    copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 247, in copyfileobj\n",
      "    buf = src.read(bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 506, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 158, in _try_build\n",
      "    model = self._build_hypermodel(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 146, in _build_hypermodel\n",
      "    model = self.hypermodel.build(hp)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\graph.py\", line 250, in build\n",
      "    outputs = block.build(hp, inputs=temp_inputs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 159, in build\n",
      "    output_node = self._build_block(hp, output_node, block_type)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py\", line 174, in _build_block\n",
      "    output_node = basic.BertBlock().build(hp, output_node)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py\", line 38, in _build_wrapper\n",
      "    return super()._build_wrapper(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py\", line 111, in _build_wrapper\n",
      "    return self._build(hp, *args, **kwargs)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\basic.py\", line 984, in build\n",
      "    bert_encoder.load_pretrained_weights()\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\keras_layers.py\", line 292, in load_pretrained_weights\n",
      "    path = keras.utils.get_file(\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 308, in get_file\n",
      "    _extract_archive(fpath, datadir, archive_format)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py\", line 137, in _extract_archive\n",
      "    archive.extractall(path)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2045, in extractall\n",
      "    self.extract(tarinfo, path, set_attrs=not tarinfo.isdir(),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2086, in extract\n",
      "    self._extract_member(tarinfo, os.path.join(path, tarinfo.name),\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2159, in _extract_member\n",
      "    self.makefile(tarinfo, targetpath)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 2208, in makefile\n",
      "    copyfileobj(source, target, tarinfo.size, ReadError, bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py\", line 247, in copyfileobj\n",
      "    buf = src.read(bufsize)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 300, in read\n",
      "    return self._buffer.read(size)\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\_compression.py\", line 68, in readinto\n",
      "    data = self.read(len(byte_view))\n",
      "  File \"c:\\Users\\ouyan\\anaconda3\\lib\\gzip.py\", line 506, in read\n",
      "    raise EOFError(\"Compressed file ended before the \"\n",
      "EOFError: Compressed file ended before the end-of-stream marker was reached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid model 5/5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Too many failed attempts to build model.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:158\u001b[0m, in \u001b[0;36mTuner._try_build\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_hypermodel(hp)\n\u001b[0;32m    159\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:146\u001b[0m, in \u001b[0;36mTuner._build_hypermodel\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m maybe_distribute(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution_strategy):\n\u001b[1;32m--> 146\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhypermodel\u001b[39m.\u001b[39;49mbuild(hp)\n\u001b[0;32m    147\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_override_compile_args(model)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:111\u001b[0m, in \u001b[0;36mHyperModel._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     hp \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 111\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build(hp, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\graph.py:250\u001b[0m, in \u001b[0;36mGraph.build\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    246\u001b[0m temp_inputs \u001b[39m=\u001b[39m [\n\u001b[0;32m    247\u001b[0m     keras_nodes[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_node_to_id[input_node]]\n\u001b[0;32m    248\u001b[0m     \u001b[39mfor\u001b[39;00m input_node \u001b[39min\u001b[39;00m block\u001b[39m.\u001b[39minputs\n\u001b[0;32m    249\u001b[0m ]\n\u001b[1;32m--> 250\u001b[0m outputs \u001b[39m=\u001b[39m block\u001b[39m.\u001b[39;49mbuild(hp, inputs\u001b[39m=\u001b[39;49mtemp_inputs)\n\u001b[0;32m    251\u001b[0m outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mflatten(outputs)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py:38\u001b[0m, in \u001b[0;36mBlock._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mwith\u001b[39;00m hp\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname):\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_build_wrapper(hp, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:111\u001b[0m, in \u001b[0;36mHyperModel._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     hp \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 111\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build(hp, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py:159\u001b[0m, in \u001b[0;36mTextBlock.build\u001b[1;34m(self, hp, inputs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[39mwith\u001b[39;00m hp\u001b[39m.\u001b[39mconditional_scope(BLOCK_TYPE, [block_type]):\n\u001b[1;32m--> 159\u001b[0m         output_node \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_block(hp, output_node, block_type)\n\u001b[0;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\wrapper.py:174\u001b[0m, in \u001b[0;36mTextBlock._build_block\u001b[1;34m(self, hp, output_node, block_type)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[39mif\u001b[39;00m block_type \u001b[39m==\u001b[39m BERT:\n\u001b[1;32m--> 174\u001b[0m     output_node \u001b[39m=\u001b[39m basic\u001b[39m.\u001b[39;49mBertBlock()\u001b[39m.\u001b[39;49mbuild(hp, output_node)\n\u001b[0;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\block.py:38\u001b[0m, in \u001b[0;36mBlock._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mwith\u001b[39;00m hp\u001b[39m.\u001b[39mname_scope(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname):\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_build_wrapper(hp, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:111\u001b[0m, in \u001b[0;36mHyperModel._build_wrapper\u001b[1;34m(self, hp, *args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m     hp \u001b[39m=\u001b[39m hp\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 111\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build(hp, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\blocks\\basic.py:984\u001b[0m, in \u001b[0;36mBertBlock.build\u001b[1;34m(self, hp, inputs)\u001b[0m\n\u001b[0;32m    983\u001b[0m output_node \u001b[39m=\u001b[39m bert_encoder(output_node)\n\u001b[1;32m--> 984\u001b[0m bert_encoder\u001b[39m.\u001b[39;49mload_pretrained_weights()\n\u001b[0;32m    986\u001b[0m \u001b[39mreturn\u001b[39;00m output_node\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\keras_layers.py:292\u001b[0m, in \u001b[0;36mBertEncoder.load_pretrained_weights\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_pretrained_weights\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     path \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mget_file(\n\u001b[0;32m    293\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mbert_checkpoint\u001b[39;49m\u001b[39m\"\u001b[39;49m, constants\u001b[39m.\u001b[39;49mBERT_CHECKPOINT_PATH, extract\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    294\u001b[0m     )\n\u001b[0;32m    295\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m    296\u001b[0m         os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, os\u001b[39m.\u001b[39mpardir)), \u001b[39m\"\u001b[39m\u001b[39mbert\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbert_ckpt-1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    297\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py:308\u001b[0m, in \u001b[0;36mget_file\u001b[1;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mif\u001b[39;00m extract:\n\u001b[1;32m--> 308\u001b[0m   _extract_archive(fpath, datadir, archive_format)\n\u001b[0;32m    310\u001b[0m \u001b[39mreturn\u001b[39;00m fpath\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras\\utils\\data_utils.py:137\u001b[0m, in \u001b[0;36m_extract_archive\u001b[1;34m(file_path, path, archive_format)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m   archive\u001b[39m.\u001b[39;49mextractall(path)\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (tarfile\u001b[39m.\u001b[39mTarError, \u001b[39mRuntimeError\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py:2045\u001b[0m, in \u001b[0;36mTarFile.extractall\u001b[1;34m(self, path, members, numeric_owner)\u001b[0m\n\u001b[0;32m   2044\u001b[0m     \u001b[39m# Do not set_attrs directories, as we will do that further down\u001b[39;00m\n\u001b[1;32m-> 2045\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract(tarinfo, path, set_attrs\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m tarinfo\u001b[39m.\u001b[39;49misdir(),\n\u001b[0;32m   2046\u001b[0m                  numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[0;32m   2048\u001b[0m \u001b[39m# Reverse sort directories.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py:2086\u001b[0m, in \u001b[0;36mTarFile.extract\u001b[1;34m(self, member, path, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2085\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2086\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extract_member(tarinfo, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(path, tarinfo\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m   2087\u001b[0m                          set_attrs\u001b[39m=\u001b[39;49mset_attrs,\n\u001b[0;32m   2088\u001b[0m                          numeric_owner\u001b[39m=\u001b[39;49mnumeric_owner)\n\u001b[0;32m   2089\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py:2159\u001b[0m, in \u001b[0;36mTarFile._extract_member\u001b[1;34m(self, tarinfo, targetpath, set_attrs, numeric_owner)\u001b[0m\n\u001b[0;32m   2158\u001b[0m \u001b[39mif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misreg():\n\u001b[1;32m-> 2159\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmakefile(tarinfo, targetpath)\n\u001b[0;32m   2160\u001b[0m \u001b[39melif\u001b[39;00m tarinfo\u001b[39m.\u001b[39misdir():\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py:2208\u001b[0m, in \u001b[0;36mTarFile.makefile\u001b[1;34m(self, tarinfo, targetpath)\u001b[0m\n\u001b[0;32m   2207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2208\u001b[0m     copyfileobj(source, target, tarinfo\u001b[39m.\u001b[39;49msize, ReadError, bufsize)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\tarfile.py:247\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(src, dst, length, exception, bufsize)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[39mfor\u001b[39;00m b \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(blocks):\n\u001b[1;32m--> 247\u001b[0m     buf \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39;49mread(bufsize)\n\u001b[0;32m    248\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buf) \u001b[39m<\u001b[39m bufsize:\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\gzip.py:300\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(errno\u001b[39m.\u001b[39mEBADF, \u001b[39m\"\u001b[39m\u001b[39mread() on write-only GzipFile object\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 300\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer\u001b[39m.\u001b[39;49mread(size)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b) \u001b[39mas\u001b[39;00m view, view\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(byte_view))\n\u001b[0;32m     69\u001b[0m     byte_view[:\u001b[39mlen\u001b[39m(data)] \u001b[39m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\gzip.py:506\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[39mif\u001b[39;00m buf \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 506\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCompressed file ended before the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    507\u001b[0m                        \u001b[39m\"\u001b[39m\u001b[39mend-of-stream marker was reached\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    509\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_add_read_data( uncompress )\n",
      "\u001b[1;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ouyan\\Desktop\\Code\\FakeNews\\main.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ouyan/Desktop/Code/FakeNews/main.ipynb#ch0000025?line=1'>2</a>\u001b[0m clf \u001b[39m=\u001b[39m ak\u001b[39m.\u001b[39mTextClassifier(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ouyan/Desktop/Code/FakeNews/main.ipynb#ch0000025?line=2'>3</a>\u001b[0m     \u001b[39m#max_trials=100,\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ouyan/Desktop/Code/FakeNews/main.ipynb#ch0000025?line=3'>4</a>\u001b[0m     \u001b[39m#overwrite = False\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ouyan/Desktop/Code/FakeNews/main.ipynb#ch0000025?line=4'>5</a>\u001b[0m ) \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ouyan/Desktop/Code/FakeNews/main.ipynb#ch0000025?line=5'>6</a>\u001b[0m \u001b[39m# Feed the text classifier with training data.\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ouyan/Desktop/Code/FakeNews/main.ipynb#ch0000025?line=6'>7</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(x_train_ak, y_train_ak, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\tasks\\text.py:160\u001b[0m, in \u001b[0;36mTextClassifier.fit\u001b[1;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    105\u001b[0m     x\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    112\u001b[0m ):\n\u001b[0;32m    113\u001b[0m     \u001b[39m\"\"\"Search for the best model and hyperparameters for the AutoModel.\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \n\u001b[0;32m    115\u001b[0m \u001b[39m    It will search for the best model based on the performances on\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[39m            validation loss values and validation metrics values (if applicable).\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m     history \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mfit(\n\u001b[0;32m    161\u001b[0m         x\u001b[39m=\u001b[39mx,\n\u001b[0;32m    162\u001b[0m         y\u001b[39m=\u001b[39my,\n\u001b[0;32m    163\u001b[0m         epochs\u001b[39m=\u001b[39mepochs,\n\u001b[0;32m    164\u001b[0m         callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    165\u001b[0m         validation_split\u001b[39m=\u001b[39mvalidation_split,\n\u001b[0;32m    166\u001b[0m         validation_data\u001b[39m=\u001b[39mvalidation_data,\n\u001b[0;32m    167\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\auto_model.py:292\u001b[0m, in \u001b[0;36mAutoModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m validation_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m validation_split:\n\u001b[0;32m    288\u001b[0m     dataset, validation_data \u001b[39m=\u001b[39m data_utils\u001b[39m.\u001b[39msplit_dataset(\n\u001b[0;32m    289\u001b[0m         dataset, validation_split\n\u001b[0;32m    290\u001b[0m     )\n\u001b[1;32m--> 292\u001b[0m history \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtuner\u001b[39m.\u001b[39msearch(\n\u001b[0;32m    293\u001b[0m     x\u001b[39m=\u001b[39mdataset,\n\u001b[0;32m    294\u001b[0m     epochs\u001b[39m=\u001b[39mepochs,\n\u001b[0;32m    295\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[0;32m    296\u001b[0m     validation_data\u001b[39m=\u001b[39mvalidation_data,\n\u001b[0;32m    297\u001b[0m     validation_split\u001b[39m=\u001b[39mvalidation_split,\n\u001b[0;32m    298\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m    299\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    300\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[39mreturn\u001b[39;00m history\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\tuner.py:193\u001b[0m, in \u001b[0;36mAutoTuner.search\u001b[1;34m(self, epochs, callbacks, validation_split, verbose, **fit_kwargs)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_build(hp)\n\u001b[0;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moracle\u001b[39m.\u001b[39mupdate_space(hp)\n\u001b[1;32m--> 193\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msearch(\n\u001b[0;32m    194\u001b[0m     epochs\u001b[39m=\u001b[39mepochs, callbacks\u001b[39m=\u001b[39mnew_callbacks, verbose\u001b[39m=\u001b[39mverbose, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    197\u001b[0m \u001b[39m# Train the best model use validation data.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# Train the best model with enough number of epochs.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39mif\u001b[39;00m validation_split \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m early_stopping_inserted:\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:183\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 183\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_trial(trial, \u001b[39m*\u001b[39mfit_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_kwargs)\n\u001b[0;32m    184\u001b[0m \u001b[39m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39mif\u001b[39;00m results \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:294\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    293\u001b[0m     copied_kwargs[\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m callbacks\n\u001b[1;32m--> 294\u001b[0m     obj_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_and_fit_model(trial, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    296\u001b[0m     histories\u001b[39m.\u001b[39mappend(obj_value)\n\u001b[0;32m    297\u001b[0m \u001b[39mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\autokeras\\engine\\tuner.py:91\u001b[0m, in \u001b[0;36mAutoTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_build_and_fit_model\u001b[39m(\u001b[39mself\u001b[39m, trial, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 91\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_build(trial\u001b[39m.\u001b[39;49mhyperparameters)\n\u001b[0;32m     92\u001b[0m     (\n\u001b[0;32m     93\u001b[0m         pipeline,\n\u001b[0;32m     94\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     95\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39mvalidation_data\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     96\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_model_build(trial\u001b[39m.\u001b[39mhyperparameters, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m     pipeline\u001b[39m.\u001b[39msave(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline_path(trial\u001b[39m.\u001b[39mtrial_id))\n",
      "File \u001b[1;32mc:\\Users\\ouyan\\anaconda3\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:166\u001b[0m, in \u001b[0;36mTuner._try_build\u001b[1;34m(self, hp)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInvalid model \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (i, MAX_FAIL_STREAK))\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m MAX_FAIL_STREAK:\n\u001b[1;32m--> 166\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mToo many failed attempts to build model.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m# Stop if `build()` does not return a valid model.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Too many failed attempts to build model."
     ]
    }
   ],
   "source": [
    "# Initialize the text classifier.\n",
    "clf = ak.TextClassifier(\n",
    "    #max_trials=100,\n",
    "    #overwrite = False\n",
    ") \n",
    "# Feed the text classifier with training data.\n",
    "clf.fit(x_train_ak, y_train_ak, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test loss, test acc\n",
    "clf.evaluate(x_test_ak, y_test_ak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preciction_tf(clf, p_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "import autokeras as ak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "    fname=\"aclImdb.tar.gz\",\n",
    "    origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\",\n",
    "    extract=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "b'Zero Day leads you to think, even re-think why two'\n"
     ]
    }
   ],
   "source": [
    "# set path to dataset\n",
    "IMDB_DATADIR = os.path.join(os.path.dirname(dataset), \"aclImdb\")\n",
    "\n",
    "classes = [\"pos\", \"neg\"]\n",
    "train_data = load_files(\n",
    "    os.path.join(IMDB_DATADIR, \"train\"), shuffle=True, categories=classes\n",
    ")\n",
    "test_data = load_files(\n",
    "    os.path.join(IMDB_DATADIR, \"test\"), shuffle=False, categories=classes\n",
    ")\n",
    "\n",
    "x_train = np.array(train_data.data)\n",
    "y_train = np.array(train_data.target)\n",
    "x_test = np.array(test_data.data)\n",
    "y_test = np.array(test_data.target)\n",
    "\n",
    "print(x_train.shape)  # (25000,)\n",
    "print(y_train.shape)  # (25000, 1)\n",
    "print(x_train[0][:50])  # this film was just brilliant casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 Complete [00h 00m 38s]\n",
      "val_loss: 0.3694274425506592\n",
      "\n",
      "Best val_loss So Far: 0.3694274425506592\n",
      "Total elapsed time: 00h 00m 38s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 4s 18ms/step - loss: 0.6855 - accuracy: 0.5282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\text_classifier\\best_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: .\\text_classifier\\best_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 9ms/step\n",
      "782/782 [==============================] - 5s 7ms/step\n",
      "782/782 [==============================] - 7s 8ms/step - loss: 0.6375 - accuracy: 0.6081\n",
      "[0.6374950408935547, 0.6080800294876099]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the text classifier.\n",
    "clf = ak.TextClassifier(\n",
    "    overwrite=True, max_trials=1\n",
    ")  # It only tries 1 model as a quick demo.\n",
    "# Feed the text classifier with training data.\n",
    "clf.fit(x_train, y_train)\n",
    "# Predict with the best model.\n",
    "predicted_y = clf.predict(x_test)\n",
    "# Evaluate the best model with testing data.\n",
    "print(clf.evaluate(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    # Split the training data and use the last 15% as validation data.\n",
    "    validation_split=0.15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 5000\n",
    "x_val = x_train[split:]\n",
    "y_val = y_train[split:]\n",
    "x_train = x_train[:split]\n",
    "y_train = y_train[:split]\n",
    "clf.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=2,\n",
    "    # Use your own validation set.\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "91750c2a53790ed423a273a56ec734ea7db5090134fb92fb4e4bee44fe7b9fce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
